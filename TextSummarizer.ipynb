{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgQu+8jA63q7qjl2AGK3Yg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abh22/textSummarizer/blob/main/TextSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M_gYdaY4uhP"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "z0OzKMSs5Gts"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading pipeline\n",
        "summarizer=pipeline('summarization')"
      ],
      "metadata": {
        "id": "okeqrdmJ5yGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"\"\"\n",
        "Transformers are a type of DNNs that offer a solution to the limitations of sequence-to-sequence (seq-2-seq) architectures, including short-term dependency of sequence inputs and the sequential processing of input, which hinders parallel training of networks. Transformers leverage the multi-head self-attention mechanism to extract features, and they exhibit great potential for application in NLP. Unlike traditional recurrence methods, Transformers utilize attention to learn from an entire segment of a sequence, using encoding and decoding blocks. One key advantage of Transformers over LSTM and RNNs is their ability to capture the true meaning of the context, owing to their attention mechanism. Moreover, Transformers are faster since they can work in parallel, unlike recurrent networks, and can be calculated using Graphic Processing Units (GPUs), allowing for faster computation of tasks with large inputs (Niu et al., 2021, Vaswani et al., 2017, Zheng, Li et al., 2020). The advantages of the Transformer model have inspired deep learning researchers to explore its potential for various tasks in different fields of application (Ren, Li, & Liu, 2023), leading to numerous research papers and the development of Transformer-based models for a range of tasks in the field of artificial intelligence (Reza et al., 2022, Wang et al., 2019, Yeh et al., 2019).\n",
        "\n",
        "In the research community, the importance of survey papers in providing a productive analysis, comparison, and contribution of progressive topics is widely recognized. Numerous survey papers on the topic of Transformers can be found in the literature. Most of them are addressing specific fields of application (Khan et al., 2022, Shamshad et al., 2023, Wang, Smetannikov et al., 2020), compare the performance of different model (Fournier et al., 2023, Selva et al., 2023, Tay et al., 2023), or conduct architecture-based analysis (Lin, Wang, Liu, & Qiu, 2022). Nevertheless, a well-defined structure that comprehensively focuses on the top application fields and systematically analyzes the contribution of Transformer-based models in the execution of various deep learning tasks within those fields is still widely needed.\n",
        "\n",
        "Indeed, conducting a survey on Transformer applications would serve as a valuable reference source for enthusiastic deep-learning researchers seeking to gain a better understanding of the contributions of Transformer models in diverse fields. Such a survey would enable the identification and discussion of potential models, their characteristics, and working methodology, thus promoting the refinement of existing Transformer models and the discovery of novel Transformer models or applications. To address the absence of such a survey, this paper presents a comprehensive analysis of all Transformer-based models, and identifies the top five application fields, namely NLP, CV, Multi-Modality, Audio & Speech, and Signal Processing, and proposes a taxonomy of Transformer models, with significant models being classified and analyzed based on their task execution within these fields. Furthermore, the top-performing and significant models are analyzed within the application fields, and based on this analysis, we discuss the future prospects and challenges of Transformer models.\n",
        "\n",
        "Although several survey articles on the topic of Transformers already exist in the literature, our motivations for conducting this survey stem from two essential observations. First, most of these studies have focused on Transformer architecture, model efficiency, and specific artificial intelligence fields, such as NLP, CV, multi-modality, audio & speech, and signal processing. They have often neglected other crucial aspects, such as the Transformer-based modelâ€™s execution in deep learning tasks across multiple application domains. We aim in this survey to cover all major fields of application and present significant models for different task executions. The second motivation is the absence of a comprehensive and methodical analysis encompassing various prevalent application domains, and their corresponding utilization of Transformer-based models, in relation to diverse deep learning tasks within distinct fields of application. We propose a high-level classification framework for Transformer models, which is based on their most prominent fields of application. The prominent models are categorized and evaluated based on their task performance within the respective fields. In this survey, we highlight the application domains of Transformers that have received comparatively greater or lesser attention from researchers. To the best of our knowledge, this is the first review paper that presents a high-level classification scheme for the Transformer-based models and provides a collection of criteria that aim to achieve two objectives: (1) assessing the effectiveness of Transformer models in various applications; and (2) assisting researchers interested in exploring and extending the capabilities of Transformer-based models to new domains. Moreover, the paper provides valuable insights into potential future applications and highlights unresolved challenges within this field.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nUILCSuA6ajE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer(text,max_length=130,min_length=30,do_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPochpeV7ANQ",
        "outputId": "0040417b-1497-4646-b8dc-f499b977a603"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' DNNs offer a solution to the limitations of sequence-to-sequence (seq-2-seq) architectures . Transformers leverage the multi-head self-attention mechanism to extract features, and they exhibit great potential for application in NLP . The advantages of the Transformer model have inspired deep learning researchers to explore its potential for various tasks in different fields of application .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install streamlit\n",
        "import streamlit as st\n",
        "\n",
        "# Your summarization function here\n",
        "def summarize(text):\n",
        "    # Summarize the text using your transformers pipeline\n",
        "    summarized_text = summarizer(text)\n",
        "    return summarized_text\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Text Summarizer\")\n",
        "user_input = st.text_area(\"Enter text to summarize:\")\n",
        "\n",
        "if st.button(\"Summarize\"):\n",
        "    summary = summarize(user_input)\n",
        "    st.write(\"Summary:\", summary)"
      ],
      "metadata": {
        "id": "9TSczTon9XyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6qVhCrI9u9a",
        "outputId": "c9d593b5-62da-4851-8a59-d284489f4765"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.90.111.230:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}